{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da46adae",
   "metadata": {},
   "source": [
    "Defino los mapas de los conjuntos y los alias para los 4 algoritmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69527ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib  # Para guardar los modelos entrenados\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Definimos los algoritmos que vamos a usar\n",
    "modelos_config = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=3), # Puedes ajustar n_neighbors\n",
    "    \"svm\": SVC(probability=True, random_state=42),\n",
    "    \"nb\": GaussianNB(),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Mapeamos los nombres de los datasets según lo que generamos en la práctica 2:\n",
    "datasets_map = {\n",
    "    # Datos Originales\n",
    "    \"original\": {\"folder\": \"Original\", \"suffix\": \"\"},\n",
    "    \"originalPCA95\": {\"folder\": \"Original\", \"suffix\": \"_pca95\"},\n",
    "    \"originalPCA80\": {\"folder\": \"Original\", \"suffix\": \"_pca80\"},\n",
    "    \n",
    "    # Datos Estandarizados\n",
    "    \"estandarizado\": {\"folder\": \"Estandarizado\", \"suffix\": \"\"},\n",
    "    \"estandarizadoPCA95\": {\"folder\": \"Estandarizado\", \"suffix\": \"_pca95\"},\n",
    "    \"estandarizadoPCA80\": {\"folder\": \"Estandarizado\", \"suffix\": \"_pca80\"},\n",
    "    \n",
    "    # Datos Normalizados\n",
    "    \"normalizado\": {\"folder\": \"Normalizado\", \"suffix\": \"\"},\n",
    "    \"normalizadoPCA95\": {\"folder\": \"Normalizado\", \"suffix\": \"_pca95\"},\n",
    "    \"normalizadoPCA80\": {\"folder\": \"Normalizado\", \"suffix\": \"_pca80\"},\n",
    "}\n",
    "\n",
    "# Carpeta donde guardaremos los modelos entrenados\n",
    "models_output_folder = \"models\"\n",
    "if not os.path.exists(models_output_folder):\n",
    "    os.makedirs(models_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0b8a3",
   "metadata": {},
   "source": [
    "Siguiente función:\n",
    "\n",
    "\n",
    "-Construye la ruta del archivo training correcto basándose en el mapa que acabamos de definir arriba\n",
    "\n",
    "-Carga los datos\n",
    "\n",
    "-Separa lo de las características (X) y las etiquetas (y)\n",
    "\n",
    "-Entrena el modelo que le entra\n",
    "\n",
    "-Guarda el modelo en la carpeta models con su nombre ej estructura  --->  model_3_normalizadoPCA95_knn.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75221ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(fold, method, dataset_name):\n",
    " \n",
    "    # La función entrena un modelo específico (method) para un pliegue (fold) y dataset (dataset_name) dados, y lo guarda en disco.\n",
    "    \n",
    "    # Identificamos el archivo de datos\n",
    "    info = datasets_map[dataset_name]\n",
    "    filename = f\"training{fold}{info['suffix']}.csv\"\n",
    "    filepath = os.path.join(info['folder'], filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: No se encuentra {filepath}\")\n",
    "        return\n",
    "\n",
    "    # Cargamos los datos asumiendoo que la última columna es la etiqueta (la clase que sea)\n",
    "    data = pd.read_csv(filepath, header=None)\n",
    "    X_train = data.iloc[:, :-1].values\n",
    "    y_train = data.iloc[:, -1].values.ravel() # ravel() para asegurar que sea un vector 1D\n",
    "\n",
    "    # Inicializamos el modelo\n",
    "    if method not in modelos_config:\n",
    "        print(f\"Método {method} no reconocido.\")\n",
    "        return\n",
    "\n",
    "    clf = modelos_config[method]\n",
    "    \n",
    "    # Entrenamos el modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Se guarda el modelo\n",
    "    model_filename = f\"model_{fold}_{dataset_name}_{method}.pkl\"\n",
    "    model_path = os.path.join(models_output_folder, model_filename)\n",
    "    \n",
    "    joblib.dump(clf, model_path)\n",
    "    # Mencionamos en el import de arriba que usamos joblib para guardar los modelos entrenados"
   ]
<<<<<<< Updated upstream
=======
  },
  {
   "cell_type": "markdown",
   "id": "ab7495dd",
   "metadata": {},
   "source": [
    "Ahora, en lugar de ejecutar comandos uno por uno, podemos hacer un bucle triple que recorra todas las combinaciones posibles y entrene todo de una vez. Esto generará todos los archivos .pkl necesarios para la siguiente fase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4492e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento masivo...\n",
      "\n",
      "¡Entrenamiento finalizado! Se han generado 180 modelos en la carpeta 'Modelos'.\n"
     ]
    }
   ],
   "source": [
    "# Bucle principal de entrenamiento\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "methods = [\"knn\", \"svm\", \"nb\", \"rf\"]\n",
    "datasets = list(datasets_map.keys())\n",
    "\n",
    "total_combinations = len(folds) * len(methods) * len(datasets)\n",
    "counter = 0\n",
    "\n",
    "for fold in folds:\n",
    "    for ds_name in datasets:\n",
    "        for method in methods:\n",
    "            train_and_save_model(fold, method, ds_name)\n",
    "            counter += 1\n",
    "            \n",
    "print(f\"\\n¡Entrenamiento finalizado! Se han generado {counter} modelos en la carpeta '{models_output_folder}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6526c",
   "metadata": {},
   "source": [
    "¿Qué hace esto? \n",
    "\n",
    "\n",
    "Automatización Total: Entrena automáticamente K-NN, SVM, Naive Bayes y Random Forests para las 5 iteraciones de validación cruzada.\n",
    "\n",
    "\n",
    "\n",
    "Cobertura de Datos: Procesa todas las variantes de datos (Original, Estandarizado, Normalizado y sus PCAs).\n",
    "\n",
    "\n",
    "Parametrización: La función train_and_save_model actúa como el sistema parametrizado que pedía el enunciado.\n",
    "\n",
    "\n",
    "Aislamiento: En cada llamada, carga el archivo trainingX específico, asegurando que el entrenamiento sea \"cerrado\" y no vea datos de test.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Persistencia: Guarda cada modelo individualmente con joblib, listo para ser cargado por el archivo eval.ipynb."
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
